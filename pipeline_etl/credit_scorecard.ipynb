{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk9jw-HaF-iZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt \n",
        "plt.rc(\"font\", size=14)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "sns.set(style=\"white\")\n",
        "sns.set(style=\"whitegrid\", color_codes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOcfvWgpF-ic",
        "outputId": "9e2587d4-774f-40aa-cf97-9d3de791c244"
      },
      "outputs": [],
      "source": [
        "df_loan = pd.read_csv('C:/Users/aksin52/Documents/My Webinars/Credit Risk Datasets and Projects/Lending Data and Project/lending-club-loan-data/loan.csv')\n",
        "#df_test  = pd.read_excel('C:/Users/aksin52/Documents/Webinar_Customer_Segmentation/test.xlsx')\n",
        "#df_combined = pd.concat([df_train, df_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fKke5WjF-id",
        "outputId": "44b20c34-ff0b-4cb3-e8e9-1c63b88d1075"
      },
      "outputs": [],
      "source": [
        "#Analyzing target variable - loan_status\n",
        "df_loan['loan_status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixerRfa7F-ie",
        "outputId": "7ea121c9-5737-4399-bf59-84a3b820fb70"
      },
      "outputs": [],
      "source": [
        "loan_filter = df_loan['loan_status'].isin(['Fully Paid','Charged Off','Default'])\n",
        "df_loan = df_loan[loan_filter]\n",
        "df_loan['loan_status'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQKydQswF-ie"
      },
      "source": [
        "Python map() function is used to apply a function on all the elements of specified iterable and return map object. \n",
        "Python map object is an iterator, so we can iterate over its elements. \n",
        "We can also convert map object to sequence objects such as list, tuple etc. using their factory functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5WMYrf3F-if",
        "outputId": "c3951ec1-2edb-4a85-cc9b-4f164f27e620"
      },
      "outputs": [],
      "source": [
        "def CreateTarget(status): \n",
        "    if status == 'Fully Paid':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "    \n",
        "df_loan['Late_Loan'] = df_loan['loan_status'].map(CreateTarget)\n",
        "df_loan['Late_Loan'].value_counts()\n",
        "df_loan['Late_Loan'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoDzo9QcF-if",
        "outputId": "95d1db11-199c-4d47-a006-77670c04f8ea"
      },
      "outputs": [],
      "source": [
        "#limitPer = len(df_loan) * .01\n",
        "#df_loan = df_loan.dropna(thresh=limitPer,axis=1)\n",
        "#len(df_loan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BqM8tQnF-ig"
      },
      "outputs": [],
      "source": [
        "#drop features with more than 10% missing values\n",
        "features_missing_series = df_loan.isnull().sum() > len(df_loan)/10\n",
        "features_missing_series = features_missing_series[features_missing_series == True]\n",
        "features_missing_list =  features_missing_series.index.tolist()\n",
        "df_loan = df_loan.drop(features_missing_list,axis =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8B0BIBsF-ig"
      },
      "outputs": [],
      "source": [
        "#list(df_loan.columns.values)\n",
        "#Drop those variables that have little impact on the outcome and does not make intuitive sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHkhv-QrF-ih",
        "outputId": "35c2591a-4688-4b28-b01b-03884c86fbef"
      },
      "outputs": [],
      "source": [
        "df_loan_1 = df_loan.drop(['id','member_id','loan_status','url','zip_code','policy_code','application_type','last_pymnt_d','last_credit_pull_d','verification_status','pymnt_plan','funded_amnt','funded_amnt_inv','sub_grade','out_prncp','out_prncp_inv','total_pymnt_inv','total_pymnt','total_pymnt_inv','total_rec_prncp','total_rec_int','total_rec_late_fee','recoveries','collection_recovery_fee','last_pymnt_amnt','initial_list_status','earliest_cr_line'],axis =1)\n",
        "df_loan_1['emp_length'].fillna('Unknown',inplace = True)\n",
        "df_loan_1['emp_title'].fillna('Unknown',inplace = True)\n",
        "df_loan_1['title'].fillna('Unknown',inplace = True)\n",
        "df_loan_1['revol_util'].fillna(df_loan_1['revol_util'].mean(),inplace = True)\n",
        "df_loan_1['collections_12_mths_ex_med'].fillna(df_loan_1['collections_12_mths_ex_med'].mean(),inplace = True)\n",
        "df_loan_1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK9hyS4cF-ih"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import re\n",
        "import traceback\n",
        "import string\n",
        "\n",
        "max_bin = 20\n",
        "force_bin = 3\n",
        "\n",
        "# define a binning function\n",
        "def mono_bin(Y, X, n = max_bin):\n",
        "    \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    r = 0\n",
        "    while np.abs(r) < 1:\n",
        "        try:\n",
        "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "            d2 = d1.groupby('Bucket', as_index=True)\n",
        "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "            n = n - 1 \n",
        "        except Exception as e:\n",
        "            n = n - 1\n",
        "\n",
        "    if len(d2) == 1:\n",
        "        n = force_bin         \n",
        "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "        if len(np.unique(bins)) == 2:\n",
        "            bins = np.insert(bins, 0, 1)\n",
        "            bins[1] = bins[1]-(bins[1]/2)\n",
        "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
        "        d2 = d1.groupby('Bucket', as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"MIN_VALUE\"] = d2.min().X\n",
        "    d3[\"MAX_VALUE\"] = d2.max().X\n",
        "    d3[\"COUNT\"] = d2.count().Y\n",
        "    d3[\"EVENT\"] = d2.sum().Y\n",
        "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "    d3=d3.reset_index(drop=True)\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def char_bin(Y, X):\n",
        "        \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
        "    df2 = notmiss.groupby('X',as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"COUNT\"] = df2.count().Y\n",
        "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "    d3[\"EVENT\"] = df2.sum().Y\n",
        "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    d3 = d3.reset_index(drop=True)\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def data_vars(df1, target):\n",
        "    \n",
        "    stack = traceback.extract_stack()\n",
        "    filename, lineno, function_name, code = stack[-2]\n",
        "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "    \n",
        "    x = df1.dtypes.index\n",
        "    count = -1\n",
        "    \n",
        "    for i in x:\n",
        "        if i.upper() not in (final.upper()):\n",
        "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "                conv = mono_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "            else:\n",
        "                conv = char_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i            \n",
        "                count = count + 1\n",
        "                \n",
        "            if count == 0:\n",
        "                iv_df = conv\n",
        "            else:\n",
        "                iv_df = iv_df.append(conv,ignore_index=True)\n",
        "    \n",
        "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "    iv = iv.reset_index()\n",
        "    return(iv_df,iv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlhTxzvyF-ik",
        "outputId": "1b6b749a-209d-49b7-f957-1e58b8d795e1"
      },
      "outputs": [],
      "source": [
        "final_iv, IV = data_vars(df_loan_1,df_loan_1.Late_Loan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvxAdmSGF-il",
        "outputId": "d2819639-a642-40e6-9e73-221fb4c5d805"
      },
      "outputs": [],
      "source": [
        "final_iv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euuYHk4EF-ip",
        "outputId": "739e18eb-1131-4fd5-f4c6-5cc758ba4c5f"
      },
      "outputs": [],
      "source": [
        "IV.sort_values('IV')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAPCQJ1lF-ip"
      },
      "outputs": [],
      "source": [
        "IV.to_csv('C:/Users/aksin52/Documents/My Webinars/Credit Risk Datasets and Projects/Lending Data and Project/lending-club-loan-data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5IVbgJ8F-iq"
      },
      "outputs": [],
      "source": [
        "transform_vars_list = df_loan_1.columns.difference(['Late_Loan'])\n",
        "transform_prefix = 'new_' # leave this value blank if you need replace the original column values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rx33I-BF-iq",
        "outputId": "bdcc4286-a73a-44cf-92d8-31b25ab1efc8"
      },
      "outputs": [],
      "source": [
        "transform_vars_list\n",
        "df=df_loan_1\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qzWK7o5F-iq"
      },
      "outputs": [],
      "source": [
        "for var in transform_vars_list:\n",
        "    small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
        "    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
        "    replace_cmd = ''\n",
        "    replace_cmd1 = ''\n",
        "    for i in sorted(transform_dict.items()):\n",
        "        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
        "        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
        "    replace_cmd = replace_cmd + '0'\n",
        "    replace_cmd1 = replace_cmd1 + '0'\n",
        "    if replace_cmd != '0':\n",
        "        try:\n",
        "            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n",
        "        except:\n",
        "            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuP5DyCgF-ir",
        "outputId": "5f44cb7b-9b53-4f77-e708-16f196427dfb"
      },
      "outputs": [],
      "source": [
        "df['new_grade'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTFd_gbvF-ir",
        "outputId": "dd8f9e83-9b48-4c78-ddf1-90b13a74c7ca"
      },
      "outputs": [],
      "source": [
        "df['grade'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT1Pf-NBF-ir",
        "outputId": "a41e6e1f-6e95-410e-f789-6ef353c57f80"
      },
      "outputs": [],
      "source": [
        "df_loan.groupby('grade').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syEaHtNcF-ir"
      },
      "outputs": [],
      "source": [
        "y=df['Late_Loan']\n",
        "y.mean()\n",
        "features=[#'new_acc_now_delinq',\n",
        "'new_addr_state',\n",
        "'new_annual_inc',\n",
        "#'new_collections_12_mths_ex_med',\n",
        "'new_delinq_2yrs',\n",
        "'new_dti',\n",
        "'new_emp_length',\n",
        "'new_grade',\n",
        "'new_home_ownership',\n",
        "'new_inq_last_6mths',\n",
        "#'new_installment',\n",
        "'new_int_rate',\n",
        "'new_issue_d',\n",
        "'new_loan_amnt',\n",
        "'new_open_acc',\n",
        "#'new_pub_rec',\n",
        "'new_purpose',\n",
        "#'new_revol_bal',\n",
        "'new_revol_util',\n",
        "'new_term'\n",
        "]\n",
        "X=df[features]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYN1WrHGF-is",
        "outputId": "9be57fc9-2ac8-4913-dbe7-884ae2127ba6"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "logit_model=sm.Logit(y,X)\n",
        "result=logit_model.fit()\n",
        "print(result.summary2())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLgTw1GQF-is"
      },
      "outputs": [],
      "source": [
        "# Dropping the non signifant varibales and re-running the earlier step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH2X_Y5_F-is",
        "outputId": "1e6a9981-b597-4b9d-d290-a0e382b9fc79"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "dtree = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=5, min_samples_leaf=5)\n",
        "dtree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhnOpp9HF-it",
        "outputId": "4e2edfda-5a93-4503-b99b-1d6e271c2cb7"
      },
      "outputs": [],
      "source": [
        "y_pred = logreg.predict(X)\n",
        "y_pred_tree=dtree.predict(X_test)\n",
        "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE1ajL4LF-it"
      },
      "outputs": [],
      "source": [
        "#Checking if the Regression Coefficients have the right sign\n",
        "#df['new_acc_now_delinq_rk']=pd.qcut(df['new_acc_now_delinq'].rank(method='first').values,10,duplicates='drop').codes+1\n",
        "#df.groupby('new_acc_now_delinq_rk')['new_acc_now_delinq','Late_Loan'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaMn7rEuF-it",
        "outputId": "a5c59fdf-bc76-4e90-a8ec-066947275045"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y, y_pred)\n",
        "#print(confusion_matrix)\n",
        "pd.crosstab(y,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aETGkrEF-it",
        "outputId": "cd4b30cc-8900-4088-c4af-3e20f32a7d60"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8AEfsfyF-iu",
        "outputId": "97f4f562-31ee-4244-dab0-e1da017f2441"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
        "tree_roc_auc=roc_auc_score(y_test, dtree.predict(X_test))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
        "fpr, tpr, thresholds = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
        "plt.plot(fpr, tpr, label='Tree Regression (area = %0.2f)' % tree_roc_auc)\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.savefig('Log_ROC')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIMoKgo7F-iu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "credit_scorecard.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
