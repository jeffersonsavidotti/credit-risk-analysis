{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração do SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/01 22:02:50 WARN Utils: Your hostname, SABRLDXV9K03 resolves to a loopback address: 127.0.1.1; using 172.30.205.65 instead (on interface eth0)\n",
      "24/11/01 22:02:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/01 22:02:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/01 22:02:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configurando o IP local para evitar o aviso de loopback\n",
    "#os.environ['SPARK_LOCAL_IP'] = '172.30.205.65'\n",
    "\n",
    "# Iniciando a SparkSession com nível de log definido para ERROR para reduzir avisos\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditRiskETL\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Ajustando para mostrar apenas erros críticos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição do Diretório Base e Criação das Pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Obter o diretório de trabalho atual\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Obter o diretório raiz do projeto (subindo um nível)\n",
    "project_root = os.path.dirname(current_dir)\n",
    "\n",
    "# Definição dos diretórios das camadas do ETL\n",
    "base_dir = os.path.join(current_dir, 'etl_camadas')\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camada Bronze: Carregamento dos Dados Brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de: /home/savidotti/programmers/credit-risk-analysis/dataset/credit_risk_dataset.csv\n",
      "\n",
      "Dados carregados com sucesso. Exibindo as primeiras linhas:\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|person_age|person_income|person_home_ownership|person_emp_length|loan_intent|loan_grade|loan_amnt|loan_int_rate|loan_status|loan_percent_income|cb_person_default_on_file|cb_person_cred_hist_length|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|        22|        59000|                 RENT|            123.0|   PERSONAL|         D|    35000|        16.02|          1|               0.59|                        Y|                         3|\n",
      "|        21|         9600|                  OWN|              5.0|  EDUCATION|         B|     1000|        11.14|          0|                0.1|                        N|                         2|\n",
      "|        25|         9600|             MORTGAGE|              1.0|    MEDICAL|         C|     5500|        12.87|          1|               0.57|                        N|                         3|\n",
      "|        23|        65500|                 RENT|              4.0|    MEDICAL|         C|    35000|        15.23|          1|               0.53|                        N|                         2|\n",
      "|        24|        54400|                 RENT|              8.0|    MEDICAL|         C|    35000|        14.27|          1|               0.55|                        Y|                         4|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Esquema do DataFrame:\n",
      "root\n",
      " |-- person_age: integer (nullable = true)\n",
      " |-- person_income: integer (nullable = true)\n",
      " |-- person_home_ownership: string (nullable = true)\n",
      " |-- person_emp_length: double (nullable = true)\n",
      " |-- loan_intent: string (nullable = true)\n",
      " |-- loan_grade: string (nullable = true)\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- loan_int_rate: double (nullable = true)\n",
      " |-- loan_status: integer (nullable = true)\n",
      " |-- loan_percent_income: double (nullable = true)\n",
      " |-- cb_person_default_on_file: string (nullable = true)\n",
      " |-- cb_person_cred_hist_length: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Camada bronze salva com sucesso em: /home/savidotti/programmers/credit-risk-analysis/pipeline_etl/etl_camadas/bronze/credit_risk_bronze.parquet\n",
      "O arquivo Parquet foi criado com sucesso em: /home/savidotti/programmers/credit-risk-analysis/pipeline_etl/etl_camadas/bronze/credit_risk_bronze.parquet\n",
      "\n",
      "Verificando os dados salvos na camada bronze:\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|person_age|person_income|person_home_ownership|person_emp_length|loan_intent|loan_grade|loan_amnt|loan_int_rate|loan_status|loan_percent_income|cb_person_default_on_file|cb_person_cred_hist_length|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|        22|        59000|                 RENT|            123.0|   PERSONAL|         D|    35000|        16.02|          1|               0.59|                        Y|                         3|\n",
      "|        21|         9600|                  OWN|              5.0|  EDUCATION|         B|     1000|        11.14|          0|                0.1|                        N|                         2|\n",
      "|        25|         9600|             MORTGAGE|              1.0|    MEDICAL|         C|     5500|        12.87|          1|               0.57|                        N|                         3|\n",
      "|        23|        65500|                 RENT|              4.0|    MEDICAL|         C|    35000|        15.23|          1|               0.53|                        N|                         2|\n",
      "|        24|        54400|                 RENT|              8.0|    MEDICAL|         C|    35000|        14.27|          1|               0.55|                        Y|                         4|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir os caminhos para cada camada\n",
    "bronze_dir = os.path.join(base_dir, 'bronze')\n",
    "silver_dir = os.path.join(base_dir, 'silver')\n",
    "gold_dir = os.path.join(base_dir, 'gold')\n",
    "\n",
    "# Criando os diretórios\n",
    "create_dir(bronze_dir)\n",
    "create_dir(silver_dir)\n",
    "create_dir(gold_dir)\n",
    "\n",
    "# Caminho para o arquivo CSV original\n",
    "raw_data_path = os.path.join(project_root, 'dataset', 'credit_risk_dataset.csv')\n",
    "\n",
    "# Verificando se o arquivo existe\n",
    "if not os.path.exists(raw_data_path):\n",
    "    print(f\"Arquivo CSV não encontrado: {raw_data_path}\")\n",
    "else:\n",
    "    print(f\"Carregando dados de: {raw_data_path}\")\n",
    "    # Carregando os dados brutos com PySpark\n",
    "    df_bronze = spark.read.csv(raw_data_path, header=True, inferSchema=True)\n",
    "    print(\"\\nDados carregados com sucesso. Exibindo as primeiras linhas:\")\n",
    "    df_bronze.show(5)  # Mostra as primeiras 5 linhas\n",
    "\n",
    "    # Exibindo o esquema do DataFrame\n",
    "    print(\"\\nEsquema do DataFrame:\")\n",
    "    df_bronze.printSchema()\n",
    "\n",
    "    # Salvando a camada bronze\n",
    "    bronze_file_path = os.path.join(bronze_dir, 'credit_risk_bronze.parquet')\n",
    "    try:\n",
    "        df_bronze.write.mode('overwrite').parquet(bronze_file_path)\n",
    "        print(f\"\\nCamada bronze salva com sucesso em: {bronze_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro ao salvar a camada bronze: {e}\")\n",
    "\n",
    "    # Verificando se o diretório Parquet foi criado\n",
    "    if os.path.isdir(bronze_file_path):\n",
    "        print(f\"O arquivo Parquet foi criado com sucesso em: {bronze_file_path}\")\n",
    "    else:\n",
    "        print(\"O arquivo Parquet não foi encontrado após a tentativa de salvamento.\")\n",
    "\n",
    "    # Opcional: Carregando e exibindo os dados salvos\n",
    "    print(\"\\nVerificando os dados salvos na camada bronze:\")\n",
    "    df_bronze_saved = spark.read.parquet(bronze_file_path)\n",
    "    df_bronze_saved.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camada Silver: Limpeza e Transformação Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tratamento de valores ausentes concluído para 'person_emp_length'.\n",
      "\n",
      "Conversão de tipos de dados concluída.\n",
      "\n",
      "Exibindo as primeiras linhas da camada silver:\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|person_age|person_income|person_home_ownership|person_emp_length|loan_intent|loan_grade|loan_amnt|loan_int_rate|loan_status|loan_percent_income|cb_person_default_on_file|cb_person_cred_hist_length|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|        22|      59000.0|                 RENT|            123.0|   PERSONAL|         D|  35000.0|        16.02|          1|               0.59|                        Y|                         3|\n",
      "|        21|       9600.0|                  OWN|              5.0|  EDUCATION|         B|   1000.0|        11.14|          0|                0.1|                        N|                         2|\n",
      "|        25|       9600.0|             MORTGAGE|              1.0|    MEDICAL|         C|   5500.0|        12.87|          1|               0.57|                        N|                         3|\n",
      "|        23|      65500.0|                 RENT|              4.0|    MEDICAL|         C|  35000.0|        15.23|          1|               0.53|                        N|                         2|\n",
      "|        24|      54400.0|                 RENT|              8.0|    MEDICAL|         C|  35000.0|        14.27|          1|               0.55|                        Y|                         4|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Esquema do DataFrame da camada silver:\n",
      "root\n",
      " |-- person_age: integer (nullable = true)\n",
      " |-- person_income: double (nullable = true)\n",
      " |-- person_home_ownership: string (nullable = true)\n",
      " |-- person_emp_length: double (nullable = false)\n",
      " |-- loan_intent: string (nullable = true)\n",
      " |-- loan_grade: string (nullable = true)\n",
      " |-- loan_amnt: double (nullable = true)\n",
      " |-- loan_int_rate: double (nullable = true)\n",
      " |-- loan_status: integer (nullable = true)\n",
      " |-- loan_percent_income: double (nullable = true)\n",
      " |-- cb_person_default_on_file: string (nullable = true)\n",
      " |-- cb_person_cred_hist_length: integer (nullable = true)\n",
      "\n",
      "\n",
      "Camada silver salva com sucesso em: /home/savidotti/programmers/credit-risk-analysis/pipeline_etl/etl_camadas/silver/credit_risk_silver.parquet\n",
      "O arquivo Parquet foi criado com sucesso em: /home/savidotti/programmers/credit-risk-analysis/pipeline_etl/etl_camadas/silver/credit_risk_silver.parquet\n",
      "\n",
      "Verificando os dados salvos na camada silver:\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|person_age|person_income|person_home_ownership|person_emp_length|loan_intent|loan_grade|loan_amnt|loan_int_rate|loan_status|loan_percent_income|cb_person_default_on_file|cb_person_cred_hist_length|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "|        22|      59000.0|                 RENT|            123.0|   PERSONAL|         D|  35000.0|        16.02|          1|               0.59|                        Y|                         3|\n",
      "|        21|       9600.0|                  OWN|              5.0|  EDUCATION|         B|   1000.0|        11.14|          0|                0.1|                        N|                         2|\n",
      "|        25|       9600.0|             MORTGAGE|              1.0|    MEDICAL|         C|   5500.0|        12.87|          1|               0.57|                        N|                         3|\n",
      "|        23|      65500.0|                 RENT|              4.0|    MEDICAL|         C|  35000.0|        15.23|          1|               0.53|                        N|                         2|\n",
      "|        24|      54400.0|                 RENT|              8.0|    MEDICAL|         C|  35000.0|        14.27|          1|               0.55|                        Y|                         4|\n",
      "+----------+-------------+---------------------+-----------------+-----------+----------+---------+-------------+-----------+-------------------+-------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando uma cópia dos dados brutos da camada bronze\n",
    "df_silver = df_bronze\n",
    "\n",
    "# Tratamento de valores ausentes em 'person_emp_length'\n",
    "median_emp_length = df_silver.approxQuantile(\"person_emp_length\", [0.5], 0.25)[0]\n",
    "df_silver = df_silver.na.fill({\"person_emp_length\": median_emp_length})\n",
    "print(\"\\nTratamento de valores ausentes concluído para 'person_emp_length'.\")\n",
    "\n",
    "# Convertendo tipos de dados para garantir consistência\n",
    "df_silver = df_silver \\\n",
    "    .withColumn(\"person_age\", col(\"person_age\").cast(IntegerType())) \\\n",
    "    .withColumn(\"person_income\", col(\"person_income\").cast(DoubleType())) \\\n",
    "    .withColumn(\"loan_amnt\", col(\"loan_amnt\").cast(DoubleType())) \\\n",
    "    .withColumn(\"loan_int_rate\", col(\"loan_int_rate\").cast(DoubleType())) \\\n",
    "    .withColumn(\"loan_status\", col(\"loan_status\").cast(IntegerType())) \\\n",
    "    .withColumn(\"loan_percent_income\", col(\"loan_percent_income\").cast(DoubleType())) \\\n",
    "    .withColumn(\"cb_person_cred_hist_length\", col(\"cb_person_cred_hist_length\").cast(IntegerType()))\n",
    "print(\"\\nConversão de tipos de dados concluída.\")\n",
    "\n",
    "# Exibindo as primeiras linhas da camada silver e o esquema do DataFrame\n",
    "print(\"\\nExibindo as primeiras linhas da camada silver:\")\n",
    "df_silver.show(5)\n",
    "print(\"\\nEsquema do DataFrame da camada silver:\")\n",
    "df_silver.printSchema()\n",
    "\n",
    "# Salvando a camada silver\n",
    "silver_file_path = os.path.join(silver_dir, 'credit_risk_silver.parquet')\n",
    "try:\n",
    "    df_silver.write.mode('overwrite').parquet(silver_file_path)\n",
    "    print(f\"\\nCamada silver salva com sucesso em: {silver_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro ao salvar a camada silver: {e}\")\n",
    "\n",
    "# Verificando se o diretório Parquet foi criado\n",
    "if os.path.isdir(silver_file_path):\n",
    "    print(f\"O arquivo Parquet foi criado com sucesso em: {silver_file_path}\")\n",
    "else:\n",
    "    print(\"O arquivo Parquet não foi encontrado após a tentativa de salvamento.\")\n",
    "\n",
    "# Carregando e exibindo os dados salvos na camada silver para verificação\n",
    "print(\"\\nVerificando os dados salvos na camada silver:\")\n",
    "df_silver_saved = spark.read.parquet(silver_file_path)\n",
    "df_silver_saved.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camada Gold: Enriquecimento e Preparação para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Arquivo silver carregado com sucesso.\n",
      "\n",
      "Tratamento de valores nulos concluído para 'person_home_ownership'.\n",
      "\n",
      "Variável 'is_home_owner' criada com sucesso.\n",
      "\n",
      "Valores nulos nas colunas numéricas foram preenchidos com valores padrão.\n",
      "\n",
      "Codificação de variáveis categóricas concluída.\n",
      "\n",
      "Vetor de features criado com sucesso.\n",
      "\n",
      "Exibindo as primeiras linhas da camada gold:\n",
      "+----------+-------------+-----------------+---------+-------------+-------------------+--------------------------+-------------+--------------------+-----+\n",
      "|person_age|person_income|person_emp_length|loan_amnt|loan_int_rate|loan_percent_income|cb_person_cred_hist_length|is_home_owner|            features|label|\n",
      "+----------+-------------+-----------------+---------+-------------+-------------------+--------------------------+-------------+--------------------+-----+\n",
      "|        22|      59000.0|            123.0|  35000.0|        16.02|               0.59|                         3|            0|(23,[0,1,2,3,4,5,...|    1|\n",
      "|        21|       9600.0|              5.0|   1000.0|        11.14|                0.1|                         2|            1|(23,[0,1,2,3,4,5,...|    0|\n",
      "|        25|       9600.0|              1.0|   5500.0|        12.87|               0.57|                         3|            1|(23,[0,1,2,3,4,5,...|    1|\n",
      "|        23|      65500.0|              4.0|  35000.0|        15.23|               0.53|                         2|            0|(23,[0,1,2,3,4,5,...|    1|\n",
      "|        24|      54400.0|              8.0|  35000.0|        14.27|               0.55|                         4|            0|(23,[0,1,2,3,4,5,...|    1|\n",
      "+----------+-------------+-----------------+---------+-------------+-------------------+--------------------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Esquema do DataFrame da camada gold:\n",
      "root\n",
      " |-- person_age: integer (nullable = false)\n",
      " |-- person_income: double (nullable = false)\n",
      " |-- person_emp_length: double (nullable = false)\n",
      " |-- loan_amnt: double (nullable = false)\n",
      " |-- loan_int_rate: double (nullable = false)\n",
      " |-- loan_percent_income: double (nullable = false)\n",
      " |-- cb_person_cred_hist_length: integer (nullable = false)\n",
      " |-- is_home_owner: integer (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "\n",
      "Camada gold salva com sucesso em: /home/savidotti/programmers/credit-risk-analysis/pipeline_etl/etl_camadas/gold/credit_risk_gold.parquet\n",
      "O arquivo Parquet foi criado com sucesso em: /home/savidotti/programmers/credit-risk-analysis/pipeline_etl/etl_camadas/gold/credit_risk_gold.parquet\n",
      "\n",
      "Verificando os dados salvos na camada gold:\n",
      "+----------+-------------+-----------------+---------+-------------+-------------------+--------------------------+-------------+--------------------+-----+\n",
      "|person_age|person_income|person_emp_length|loan_amnt|loan_int_rate|loan_percent_income|cb_person_cred_hist_length|is_home_owner|            features|label|\n",
      "+----------+-------------+-----------------+---------+-------------+-------------------+--------------------------+-------------+--------------------+-----+\n",
      "|        22|      59000.0|            123.0|  35000.0|        16.02|               0.59|                         3|            0|(23,[0,1,2,3,4,5,...|    1|\n",
      "|        21|       9600.0|              5.0|   1000.0|        11.14|                0.1|                         2|            1|(23,[0,1,2,3,4,5,...|    0|\n",
      "|        25|       9600.0|              1.0|   5500.0|        12.87|               0.57|                         3|            1|(23,[0,1,2,3,4,5,...|    1|\n",
      "|        23|      65500.0|              4.0|  35000.0|        15.23|               0.53|                         2|            0|(23,[0,1,2,3,4,5,...|    1|\n",
      "|        24|      54400.0|              8.0|  35000.0|        14.27|               0.55|                         4|            0|(23,[0,1,2,3,4,5,...|    1|\n",
      "+----------+-------------+-----------------+---------+-------------+-------------------+--------------------------+-------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Coluna 'loan_int_rate' confirmada na camada silver e propagada para a camada gold.\n"
     ]
    }
   ],
   "source": [
    "# Inicializando a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreditRiskETL\").getOrCreate()\n",
    "\n",
    "# Definindo os caminhos para as camadas silver e gold\n",
    "project_root = os.getcwd()\n",
    "silver_dir = os.path.join(project_root, 'etl_camadas', 'silver')\n",
    "gold_dir = os.path.join(project_root, 'etl_camadas', 'gold')\n",
    "silver_file_path = os.path.join(silver_dir, 'credit_risk_silver.parquet')\n",
    "gold_file_path = os.path.join(gold_dir, 'credit_risk_gold.parquet')\n",
    "\n",
    "# Verificação da camada silver\n",
    "if not os.path.exists(silver_file_path):\n",
    "    print(f\"Arquivo silver não encontrado: {silver_file_path}\")\n",
    "else:\n",
    "    df_silver = spark.read.parquet(silver_file_path)\n",
    "    print(\"\\nArquivo silver carregado com sucesso.\")\n",
    "\n",
    "    # Criando uma cópia dos dados da camada silver para a camada gold\n",
    "    df_gold = df_silver\n",
    "\n",
    "    # Tratamento de valores nulos em 'person_home_ownership'\n",
    "    df_gold = df_gold.na.fill({\"person_home_ownership\": \"OTHER\"})\n",
    "    print(\"\\nTratamento de valores nulos concluído para 'person_home_ownership'.\")\n",
    "\n",
    "    # Criar uma variável binária para indicar se a pessoa possui casa própria\n",
    "    df_gold = df_gold.withColumn(\n",
    "        'is_home_owner',\n",
    "        when(col('person_home_ownership').isin('OWN', 'MORTGAGE'), 1).otherwise(0)\n",
    "    )\n",
    "    print(\"\\nVariável 'is_home_owner' criada com sucesso.\")\n",
    "\n",
    "    # Garantindo que todas as colunas de features estejam sem nulos\n",
    "    df_gold = df_gold.na.fill({\n",
    "        \"person_age\": 0, \n",
    "        \"person_income\": 0.0,\n",
    "        \"person_emp_length\": 0.0,\n",
    "        \"loan_amnt\": 0.0,\n",
    "        \"loan_int_rate\": 0.0,\n",
    "        \"loan_percent_income\": 0.0,\n",
    "        \"cb_person_cred_hist_length\": 0,\n",
    "        \"is_home_owner\": 0\n",
    "    })\n",
    "    print(\"\\nValores nulos nas colunas numéricas foram preenchidos com valores padrão.\")\n",
    "\n",
    "    # Codificação de variáveis categóricas usando StringIndexer e OneHotEncoder\n",
    "    categorical_vars = ['loan_intent', 'loan_grade', 'cb_person_default_on_file']\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid='keep') for col in categorical_vars]\n",
    "    encoders = [OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_vec\") for col in categorical_vars]\n",
    "\n",
    "    # Aplicando o Pipeline\n",
    "    pipeline = Pipeline(stages=indexers + encoders)\n",
    "    df_gold = pipeline.fit(df_gold).transform(df_gold)\n",
    "    print(\"\\nCodificação de variáveis categóricas concluída.\")\n",
    "\n",
    "    # Criando o vetor de features\n",
    "    assembler_inputs = [\n",
    "        'person_age',\n",
    "        'person_income',\n",
    "        'person_emp_length',\n",
    "        'loan_amnt',\n",
    "        'loan_int_rate',\n",
    "        'loan_percent_income',\n",
    "        'cb_person_cred_hist_length',\n",
    "        'is_home_owner'\n",
    "    ] + [encoder.getOutputCol() for encoder in encoders]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "    df_gold = assembler.transform(df_gold)\n",
    "    print(\"\\nVetor de features criado com sucesso.\")\n",
    "\n",
    "    # Selecionando as colunas necessárias para a camada gold, incluindo as colunas originais e de features\n",
    "    df_gold = df_gold.select(\n",
    "        'person_age', \n",
    "        'person_income', \n",
    "        'person_emp_length', \n",
    "        'loan_amnt', \n",
    "        'loan_int_rate', \n",
    "        'loan_percent_income', \n",
    "        'cb_person_cred_hist_length', \n",
    "        'is_home_owner', \n",
    "        'features', \n",
    "        col('loan_status').alias('label')\n",
    "    )\n",
    "\n",
    "    # Exibindo as primeiras linhas da camada gold e o esquema do DataFrame\n",
    "    print(\"\\nExibindo as primeiras linhas da camada gold:\")\n",
    "    df_gold.show(5)\n",
    "    print(\"\\nEsquema do DataFrame da camada gold:\")\n",
    "    df_gold.printSchema()\n",
    "\n",
    "    # Salvando a camada gold\n",
    "    os.makedirs(gold_dir, exist_ok=True)\n",
    "    try:\n",
    "        df_gold.write.mode('overwrite').parquet(gold_file_path)\n",
    "        print(f\"\\nCamada gold salva com sucesso em: {gold_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro ao salvar a camada gold: {e}\")\n",
    "\n",
    "    # Verificando se o diretório Parquet foi criado\n",
    "    if os.path.isdir(gold_file_path):\n",
    "        print(f\"O arquivo Parquet foi criado com sucesso em: {gold_file_path}\")\n",
    "    else:\n",
    "        print(\"O arquivo Parquet não foi encontrado após a tentativa de salvamento.\")\n",
    "\n",
    "    # Carregando e exibindo os dados salvos na camada gold para verificação\n",
    "    print(\"\\nVerificando os dados salvos na camada gold:\")\n",
    "    df_gold_saved = spark.read.parquet(gold_file_path)\n",
    "    df_gold_saved.show(5)\n",
    "\n",
    "    # Verificação final para garantir que 'loan_int_rate' está presente na camada gold\n",
    "    if 'loan_int_rate' in df_silver.columns:\n",
    "        print(\"\\nColuna 'loan_int_rate' confirmada na camada silver e propagada para a camada gold.\")\n",
    "    else:\n",
    "        print(\"\\nA coluna 'loan_int_rate' não foi encontrada na camada silver; revise o pipeline ETL.\")\n",
    "\n",
    "# Finalizando a SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
